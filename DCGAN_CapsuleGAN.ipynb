{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_CapsuleGAN",
      "provenance": [],
      "authorship_tag": "ABX9TyOe9JwZRL2PE2vYCeeTJ2jX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wildonion/Deep-Learning_GAN/blob/master/DCGAN_CapsuleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xQsz_rda-3X",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Capsule & Deep Convolutional GAN**\n",
        "\n",
        "***Powered by:***\n",
        "\n",
        "![uniXerr logo](https://drive.google.com/uc?id=1TXJwfJsTJzU2M7LrIQgx2Tx4cfUzcQuX)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzuZYGjseftQ",
        "colab_type": "text"
      },
      "source": [
        "**Deep Convolutioanl GAN  several improvements:**\n",
        "\n",
        "*   Utilizing the convolution layer instead pooling function in the Discriminator\n",
        "model for reducing dimensionality. This way, the network itself will learn how to reduce dimensionality. On the other hand, in the Generator Model, we use deconvolution to upsample dimensions of feature maps.\n",
        "*   Adding in the batch normalization. This is used to increase the stability of a neural network. In an essence, batch normalization normalizes the output of a previous layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
        "*   Remove fully connected layers from Convolutional Neural Network.\n",
        "*   Use Relu and Leaky Relu activation functions.\n",
        "\n",
        "![DCGAN job](https://drive.google.com/uc?id=1Ind08ydejfh6IYYl6Gw_jYfLEGN4Eiph)\n",
        "\n",
        "![DCGAN process](https://drive.google.com/uc?id=1wZufkk6jq22l15a8VUFEQ1MT5mfERMS_)\n",
        "\n",
        "**Discriminator Process**\n",
        "\n",
        "> Strided convolution instead of max-pooling down samples the image.\n",
        "\n",
        "![D_process](https://drive.google.com/uc?id=1mQSjU2KVzOEQwx5qdp7VTglx5AvUhd_3)\n",
        "\n",
        "**Generator Process**\n",
        "\n",
        "> Upsampling is used instead of fractionally-strided transposed convolution.\n",
        "\n",
        "![G_process](https://drive.google.com/uc?id=1AWKUP8dGW8xdXVX8JENavoBR_JSR01WA)\n",
        "\n",
        "**Adversarial Network**\n",
        "\n",
        "> The Adversarial model is simply generator with its output connected to the input of the discriminator. Also shown is the training process wherein the Generator labels its fake image output with 1.0 trying to fool the Discriminator.\n",
        "\n",
        "![G_process](https://drive.google.com/uc?id=1jMhMV5kiaCqNa9x1E-YDdTTB4CWB8XOD)\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "> Discriminator in GAN uses a cross entropy loss, since discriminators job is to classify; cross entropy loss is the best one out there.\n",
        "\n",
        "![gan loss](https://drive.google.com/uc?id=1TZlEihIaUqK4v8_MFYb8Ilf9o0Rjw2PR)\n",
        "\n",
        "> This formula represents the cross entropy loss between `p`: the true distribution and `q`: the estimated distribution.\n",
        "`(p)` and `(q)` are the of `m` dimensions where `m` is the number of classes.\n",
        "\n",
        "![cross entropy](https://drive.google.com/uc?id=1BJSC-RUODhllXGDR6TnzuRkYBwKjg0xF)\n",
        "\n",
        "> In GAN, discriminator is a binary classifier. It needs to classify either the data is real or fake. Which means `m = 2`. The true distribution is one hot vector consisting of only 2 terms.\n",
        "For `n` number of samples, we can sum over the losses.\n",
        "This above shown equation is of binary cross entropy loss, where `y` can take two values 0 and 1.\n",
        "GAN’s have a latent vector `z`, image `G(z)` is magically generated out of it. We apply the discriminator function `D` with real image `x` and the generated image `G(z)`.\n",
        "The intention of the loss function is to push the predictions of the real image towards 1 and the fake images to 0. We do so by log probability term.\n",
        "\n",
        "![minmax formula](https://drive.google.com/uc?id=1Ky3cfOdWT1tRNk3SLT7Luscko1e3J0NT)\n",
        "\n",
        "**Note:** `~` sign means: is distributed as and `Ex` here means expectations: since we don’t know how samples are fed into the discriminator, we are representing them as expectations rather than the sum.\n",
        "If we observe the joint loss function we are maximizing the discriminator term, which means log of `D(x)` should inch closer to zero, and log `D(G(z))` should be closer to 1. Here generator is trying to make `D(G(z))` inch closer to 1 while discriminator is trying to do the opposite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd8NjI6DcyPS",
        "colab_type": "text"
      },
      "source": [
        "# **Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6vzKWsc4aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmYNjWtfiS5p",
        "colab_type": "text"
      },
      "source": [
        "# **TPU Checking**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2-A3AVeiw5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.tensorflow.org/guide/tpu\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'Did you forget to switch to TPU?'\n",
        "tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR'] # colab is using grpc for its VPSes\n",
        "\n",
        "print(f\"Found TPU at {tpu_address}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QcizkWz1LWsU"
      },
      "source": [
        "# **Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YW10oNaqLWp8",
        "colab": {}
      },
      "source": [
        "\n",
        "# Importing Modules - run twice!\n",
        "\n",
        "from __future__ import print_function, division\n",
        "try:\n",
        "    from PIL import Image\n",
        "    from tqdm import tqdm\n",
        "    import pprint, numpy as np, pandas as pd, matplotlib.pyplot as plt, os, sys, imageio, tensorflow as tf, plotly.graph_objects as go, hdbscan, cv2, asyncio, math, seaborn as sns, tensorflow.keras.backend as K\n",
        "    from tensorflow.keras.layers import MaxPooling2D, Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2DTranspose, Conv2D\n",
        "    from tensorflow.keras.models import Sequential, Model, load_model\n",
        "    from tensorflow.keras.optimizers import Adam, RMSprop \n",
        "    from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "except:\n",
        "    print(\"\\n\\n[!] Installing Dependencies...\")\n",
        "    !pip install hdbscan\n",
        "    !pip install tqdm\n",
        "finally:\n",
        "    print(\"\\n\\n[+] All Modules Loaded Successfully!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u5wXql5vxdb0"
      },
      "source": [
        "# **The Image Processing Kit**\n",
        "\n",
        "***References :***\n",
        "\n",
        "[IMP colab](https://colab.research.google.com/drive/1W79kelri9bXpLsnMtas79fmzLIgWC5bN#scrollTo=aZCfZlN42gfr&line=2&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFbUpUy-cayI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class IPKit:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.drivePath = '/gdrive/My Drive/'\n",
        "\n",
        "    def saveGANIMG(self, generated, epoch, dataset_name):\n",
        "        if not os.path.isdir('generated'): os.mkdir('generated') \n",
        "        fig, axs = plt.subplots(5, 5) # (5 , 5) images for 25 noises\n",
        "        batch_count = 0\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                # plot image on each of 25 axis of figure object in range [0, 255]\n",
        "                if dataset_name == 'paint_art' or dataset_name == 'cifar10':\n",
        "                    axs[row, col].imshow((generated[batch_count, :, :, :] * 127.5 + 127.5).astype(np.uint8))\n",
        "                else:\n",
        "                    axs[row, col].imshow(generated[batch_count, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "                axs[row, col].axis('off') # hide the related axis\n",
        "                batch_count += 1 # get ready for next data row\n",
        "        fig.savefig(f\"generated/{epoch}.png\")\n",
        "        plt.close()\n",
        "    \n",
        "    def MakeGif(self):\n",
        "        filenames = [ fname for fname in np.sort(os.listdir('generated')) if \".png\" in fname]\n",
        "        with imageio.get_writer('generated/dcgan.gif', mode=\"I\") as writer: # open a writer object for writing images on it to export a gif\n",
        "            for filename in filenames: # for every file in filenames list read them\n",
        "                image = imageio.imread('generated/'+filename)\n",
        "                writer.append_data(image) # append opened image into writer object for making gif\n",
        "\n",
        "    # call below function whenever you have new images in gdrive art folder\n",
        "    # turn all images into a numpy array of pixels\n",
        "    def buildPaint(self):\n",
        "        training_data = []\n",
        "        filenames = os.listdir(self.drivePath+'Art-Dataset/')\n",
        "        for fname in filenames:\n",
        "            image_path = os.path.join(self.drivePath+'Art-Dataset/'+fname)\n",
        "            image = Image.open(image_path).resize((128,128), Image.ANTIALIAS)\n",
        "            if np.asarray(image).shape != (128, 128, 3):\n",
        "                os.remove(image_path)\n",
        "            else:\n",
        "                training_data.append(np.asarray(image))\n",
        "                # plt.imshow(image)\n",
        "                # print(image)\n",
        "        np.save(self.drivePath+'Art-Dataset/paint_art.npy', training_data)\n",
        "    \n",
        "    def loadPaint(self):\n",
        "        return tf.data.Dataset.from_tensor_slices(np.load(os.path.join(self.drivePath+'Art-Dataset/paint_art.npy')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAk6M5vReKG",
        "colab_type": "text"
      },
      "source": [
        "# **Tooling Classes - Setting up Hyper-Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJEfo1z8RwHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HPARAM:\n",
        "    loss = 'binary_crossentropy'\n",
        "    optimizer = lambda name : Adam(lr=0.0002, beta_1=0.5) if name == 'Adam' else RMSprop(learning_rate=0.0008, rho=1.0, decay=6e-8) \n",
        "    batch_size = 64\n",
        "    buffer_size = 10000\n",
        "    epochs = 30000\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    dropout = 0.4\n",
        "    momentum = 0.9\n",
        "    depth = 256\n",
        "    metrics = ['accuracy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tisM55IQPln8",
        "colab_type": "text"
      },
      "source": [
        "# **GAN Class Constructor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANRygiBcP1Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN:\n",
        "    '''\n",
        "    we convert training images to float32 type\n",
        "    then normalize and scale the pixel data by half of the 255:\n",
        "        the activation function of the output layer of the generator is tanh, \n",
        "        which returns a value between -1 and 1. To scale that to 0 and 255 \n",
        "        (which are the values you expect for an image), we have to multiply it \n",
        "        by 127.5 (so that -1 becomes -127.5, and 1 becomes 127.5), and then \n",
        "        add 127.5 (so that -127.5 becomes 0, and 127.5 becomes 255). We then \n",
        "        have to do the inverse of this when feeding an image into the \n",
        "        discriminator (which will expect a value between -1 and 1).\n",
        "    \n",
        "    Leaky ReLUs are one attempt to fix the “dying ReLU” problem. \n",
        "    Instead of the function being zero when x < 0, a leaky ReLU \n",
        "    will instead have a small negative slope (of 0.01, or so). \n",
        "    That is, the function computes f(x)=1(x<0)(αx)+1(x>=0)(x) where α is a small constant.\n",
        "\n",
        "            NOTE : the None in models' summary is the batch dimension.\n",
        "            NOTE : \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "            NOTE : all kernels or filters in each Conv2D layers has the depth of the image channels.\n",
        "            NOTE : BatchNormalization layer normalize the activations of the previous layer at each batch, \n",
        "                   i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
        "    '''\n",
        "    def __init__(self, dataset_name='paint_art', channels=3, generator_input_features=100, discNetwork='dcgan'):\n",
        "            self.channels = channels # the default is 3, because paint dataset has colorful images \n",
        "            self.dataset_name = dataset_name\n",
        "            self.ipkit = IPKit()\n",
        "            self.dcganFlag, self.capsuleFlag = False, False \n",
        "            \n",
        "            if self.dataset_name == 'mnist' or self.dataset_name == 'fashion_mnist':\n",
        "                self.h = self.w = 28\n",
        "                self.mDep = int(self.h/4)\n",
        "\n",
        "            if discNetwork == 'dcgan':\n",
        "                self.dcganFlag = True\n",
        "            if discNetwork == 'cgan':\n",
        "                self.capsuleFlag = True\n",
        "            \n",
        "\n",
        "            if self.dataset_name == 'cifar10':\n",
        "                self.h = self.w = 32\n",
        "                self.mDep = int(self.h/8) # for generator : image length and width start from 4 up to 32 by each Conv2D layer strides and for discriminator start from 32 down to flatten 2*2*256 neurons\n",
        "                (self.x_train, _), (_, _) = cifar10.load_data()\n",
        "                self.x_train = np.reshape(self.x_train, (-1, self.h, self.w, self.channels)) # shape : (50000, 32, 32, 3)\n",
        "            elif self.dataset_name == 'mnist':\n",
        "                (self.x_train, _), (_, _) = mnist.load_data()\n",
        "                self.x_train = np.expand_dims(self.x_train, axis=3) # shape : (60000, 28, 28, 1) ; you can also use self.x_train.reshape(self.x_train.shape[0], self.h, self.w, 1)\n",
        "            elif self.dataset_name == 'fashion_mnist':\n",
        "                (self.x_train, _), (_, _) = fashion_mnist.load_data()\n",
        "                self.x_train = np.expand_dims(self.x_train, axis=3) # shape : (60000, 28, 28, 1) ; you can also use self.x_train.reshape(self.x_train.shape[0], self.h, self.w, 1)\n",
        "            elif self.dataset_name == 'paint_art':\n",
        "                self.h = self.w = 128\n",
        "                self.mDep = int(self.h/32)\n",
        "                self.x_train = self.ipkit.loadPaint()\n",
        "                self.x_train = self.x_train.shuffle(HPARAM.buffer_size, reshuffle_each_iteration=True)\n",
        "                self.x_train = self.x_train.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=10)\n",
        "                self.x_train = self.x_train.batch(HPARAM.batch_size)\n",
        "                self.x_train = self.x_train.prefetch(50) # prefetch 50 batches of HPARAM.batch_size examples each - preparing batches to feed into the next iteration\n",
        "                self.x_train = list(self.x_train.as_numpy_iterator()) # len(self.x_train) iterations wich contains HPARAM.batch_size sammples in each iter to complete one epoch\n",
        "                self.x_train.pop() # pop the last batch cause it has size of 29 - 8320 images devided into 8320/HPARAM.batch_size batches each of size HPARAM.batch_size\n",
        "                self.discriminator_input = (128, 128, 3)\n",
        "\n",
        "\n",
        "\n",
        "            if self.dataset_name is not 'paint_art':\n",
        "                self.x_train = (self.x_train.astype(np.float32) - 127.5)/127.5 # normalize the images to [-1, 1] - because the output of our generator is squashed by a tanh activation function which give a number in range [-1, 1] \n",
        "                self.discriminator_input = self.x_train[0].shape # Example : (32, 32, 1) for paint_art mnist\n",
        "                \n",
        "            \n",
        "            self.generator_input_features = generator_input_features\n",
        "            self.__create_networks()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0-RkTOPTMz",
        "colab_type": "text"
      },
      "source": [
        "# **Making Generator Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaWTKmZaPP_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def __MakeGeneratorModel(self):\n",
        "        '''\n",
        "        creating generator layers activated by tanh.\n",
        "        basically this model generates noisy images for first rounds and real images at the end of total epochs.\n",
        "        \n",
        "        The generator model is typically implemented using a deep convolutional neural network \n",
        "        and results-specialized layers that learn to fill in features in an image \n",
        "        rather than extract features from an input image, cause we want to produce a real image\n",
        "        from noisy one by learning the features map (deconvolutional process).\n",
        "\n",
        "        GAN architecture are required to upsample input data in order since it synthesizes more realistic images.\n",
        "\n",
        "        fractional stride (deconvolutional layers) can be used in the generator for upsampling.\n",
        "        \n",
        "        The upsampling layer is a simple layer with no weights that will double the dimensions of \n",
        "        input and can be used in a generative model when followed by a traditional convolutional layer.\n",
        "\n",
        "        NOTE : in order to understand the architecture of the generator model see its summary and the shape of training images!\n",
        "        NOTE : the output shape of Conv2DTranspose with padding same is : output = input * stride\n",
        "        NOTE : the default stride is 1 because we choosed to use UpSampling2D layer.\n",
        "        NOTE : you can comment UpSampling2D layers, set strides=2 for each Conv2DTranspos layers to get the same result with \n",
        "               UpSampling2D layer cause transposed convolutions are more flexible than classical upsampling methods.\n",
        "        NOTE : default strides option of Conv2DTranspose layer doesn't affect the output shape because the argument is set to 1 by default and we have UpSampling2D layer.\n",
        "               you can uncomment the Upsampling2D layers and comment Conv2DTranspose layers or set the strides of each Conv2DTranspose layers to 1 to get the benefits of upsampling method.\n",
        "        '''\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ GENERATOR SUMMARY - FEATURES/NEURONS/INPUTS STRUCTURE +)+)+)+)+)+) \\n\\n\")\n",
        "        generator_input_features = Input(shape=(self.generator_input_features,), name='generator_input_features') # create a Input layer with size for example 100 (first layer neurons)\n",
        "        self.generator = Sequential() # create sequential model object - generator/decoder\n",
        "        self.generator.add(Dense(HPARAM.depth * self.mDep * self.mDep, input_dim=self.generator_input_features)) # size of next layer (hidden) is (None, HPARAM.depth * self.mDep * self.mDep) with the input : (None, 100) - weights matrix size : (100, HPARAM.depth * self.mDep * self.mDep)\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        self.generator.add(Reshape((self.mDep, self.mDep, HPARAM.depth))) # reshape to (None, self.mDep, self.mDep, 256) - None is the batch size dim\n",
        "        self.generator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting input features to zero for perviouse layer to prevent over-fitting\n",
        "        # self.generator.add(UpSampling2D()) # opposite of pooling layer - doubles the dimensions of the last layer output ; output size : (None, 2*self.mDep, 2*self.mDep, 256)\n",
        "        \n",
        "        for n_layer in range(int(math.log2(self.h/self.mDep))):\n",
        "            self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth), kernel_size=5, strides=2, padding='same')) # output size : double the pervious layer output in every iteration by strides with filter 256\n",
        "            self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.generator.add(LeakyReLU())\n",
        "            # self.generator.add(UpSampling2D()) # double the last output size, not the filter! before use it remove strides in Conv2DTranspose layer\n",
        "\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/2), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 128) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 128 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/4), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 64) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 64 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/8), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 32) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 32 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/16), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 16) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 16 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        self.generator.add(Conv2DTranspose(filters=self.channels, kernel_size=5, padding=\"same\")) # image channels as the number of filters of the last layer - output size : (None, self.h, self.w, self.channels) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with self.channels filters\n",
        "        self.generator.add(Activation(\"tanh\")) # -1 < output < 1\n",
        "        self.generator.summary()\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ GENERATOR MODEL SUMMARY AFTER TURNING IT INTO A TENSOR +)+)+)+)+)+) \\n\\n\")\n",
        "        generator_output_tensor = self.generator(generator_input_features) # turn our generator sequential model object into a tensor with input layer for example 100 neurons - output size : (None, self.h, self.w, self.channels)\n",
        "        self.generator_model = Model(generator_input_features, generator_output_tensor) # create the generator model with for example 100 inputs and (None, self.h, self.w, self.channels) output\n",
        "        # self.generator_model.compile(loss=HPARAM.loss, optimizer=HPARAM.optimizer('Adam'), metrics=HPARAM.metrics)\n",
        "        self.generator_model.summary()\n",
        "        print(f\"\\n\\n\\t\\t [======Generator Tensor======] \\n\\n\\t\\t {generator_output_tensor}\\n\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AT0-6mYP5vb",
        "colab_type": "text"
      },
      "source": [
        "# **Making & Compiling Discriminator Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qwAB727QDZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def __MakeDiscriminatorModel(self):\n",
        "        '''\n",
        "        The discriminator model takes an example from the domain as input (real or generated)\n",
        "        and predicts a binary class label of real or fake (generated). It has kinda supervised manner!\n",
        "        \n",
        "        we use downsampling in the discriminator model to reduce dimensionality.\n",
        "        \n",
        "        In GANs, the recommendation is to not use pooling layers, \n",
        "        and instead use the stride in convolutional layers to \n",
        "        perform downsampling in the discriminator model.\n",
        "        \n",
        "        for the output layer we'll use sigmoid activation function to\n",
        "        squashes the output to a range between 0 and 1 for discriminating images.\n",
        "\n",
        "        NOTE : in order to understand the architecture of the discriminator model see its summary and the shape of training images!\n",
        "        NOTE : Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', strides=2)\n",
        "               if padding == 'same':\n",
        "                   output_length = input_length\n",
        "               elif padding == 'valid':\n",
        "                   output_length = input_length - filter_size\n",
        "               return (output_length + stride - 1) // stride\n",
        "               so (input=400 + 2-1)//2 = (200,200) will be (H,W) respectively and including filter (200,200,filter)\n",
        "        NOTE : you can remove the strides argument from each Conv2D layer and use MaxPooling2D with pool_size=2 layer to half the size of the width and height of the input features.\n",
        "               just remember to use a MaxPooling2D layer as the first layer of the discriminator using functional model api to half the size of the input features : maxpooling((None, self.h, self.w, self.channels)) -> Conv2D(32, 5, \"same\") -> (None, self.h/2, self.w/2, 32)\n",
        "        '''\n",
        "        discriminator_input = Input(shape=self.discriminator_input, name='discriminator_input_features') # create the input layer with size for example (self.h, self.w, self.channels)\n",
        "        if self.dcganFlag:\n",
        "            print(\"\\n\\n (+(+(+(+(+(+ DEEP CONVOLUTIONAL DISCRIMINATOR SUMMARY - FEATURES/NEURONS/INPUTS STRUCTURE +)+)+)+)+)+) \\n\\n\")\n",
        "            self.discriminator = Sequential() # discriminator/encoder\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/16), kernel_size=5, strides=2, input_shape=self.discriminator_input, padding=\"same\")) # output size : (None, self.h/2, self.w/2, 16) of first hidden layer - input size : (None, self.h, self.w, self.channels) >>> input_width & input_height = self.h / strides with 16 filters \n",
        "            self.discriminator.add(LeakyReLU(0.2)) # fix the “dying ReLU” problem by alpha = 0.2\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting inputs features to zero for perviouse layer and to each element or cell within the feature maps\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/8), kernel_size=5, strides=2, padding=\"same\")) # output size : half the pervious layer output by strides with 32 filters\n",
        "            \n",
        "            if self.h == 28:\n",
        "                self.discriminator.add(ZeroPadding2D(padding=((0,1),(0,1)))) # add rows and columns of zeros at the top, bottom, left and right side of an image tensor - output size : (None, (self.h/4)+1, (self.w/4)+1, 32) \n",
        "            \n",
        "            if self.h == 128:\n",
        "                for n_layer in range(2):\n",
        "                    self.discriminator.add(Conv2D(filters=int(HPARAM.depth/8), kernel_size=5, strides=2, padding=\"same\")) # output size : half the pervious layer output in every iteration by strides with 32 filters\n",
        "                    self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "                    self.discriminator.add(LeakyReLU(0.2))\n",
        "                    self.discriminator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting input features to zero for perviouse layer and to each element or cell within the feature maps to prevent over-fitting \n",
        "\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/4), kernel_size=5, strides=2, padding=\"same\")) # output size : (None, 4, 4, 64) \n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout)) \n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/4), kernel_size=5, strides=2, padding=\"same\")) # output size : (None, 2, 2, 64) \n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout))\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/2), kernel_size=5, strides=1, padding=\"same\")) # output size : (None, 2, 2, 128)\n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout))\n",
        "            self.discriminator.add(Conv2D(filters=HPARAM.depth, kernel_size=5, strides=1, padding=\"same\")) # output size : (None, 2, 2, 256) >>>\n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting inputs features to zero for perviouse layer and to each element or cell within the feature maps\n",
        "            self.discriminator.add(Flatten()) # turn the last layer by flatten it into a fully dense connected for prediction in the next - output size : (None, 1024)\n",
        "            self.discriminator.add(Dense(1, activation='sigmoid')) # one neuron (single scalar) at the output ; means the image is real or fake, 1 for real (if the sigmoid neuron's output is larger than or equal to 0.5) and 0 for fake (if the output is smaller than 0.5) - output size : (None, 1) | weights matrix size : (1024, 1)\n",
        "            self.discriminator.summary()\n",
        "        if self.capsuleFlag:\n",
        "            print(\"\\n\\n (+(+(+(+(+(+ CAPSULE DISCRIMINATOR SUMMARY - FEATURES/NEURONS/INPUTS STRUCTURE +)+)+)+)+)+) \\n\\n\")\n",
        "            x = Conv2D(filters=HPARAM.depth, kernel_size=9 , strides=1, padding='valid', name='conv1')(discriminator_input)\n",
        "            x = LeakyReLU(0.2)(x)\n",
        "            x = BatchNormalization(momentum=HPARAM.momentum)(x)\n",
        "            # ============================================================\n",
        "            # capsule network architecture only for discriminatring images\n",
        "            # https://github.com/gusgad/capsule-GAN/blob/master/capsule_gan.ipynb\n",
        "            # ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ DISCRIMINATOR MODEL SUMMARY AFTER TURNING IT INTO A TENSOR +)+)+)+)+)+) \\n\\n\")\n",
        "        discriminator_tensor = self.discriminator(discriminator_input) # turn our discriminator sequential model object into a tensor with an input layer for example (None, self.h, self.w, self.channels) inputs or neurons - output size : (None , 1)\n",
        "        self.discriminator_model = Model(discriminator_input, discriminator_tensor) # create the discriminator model with for example (None, self.h, self.w, self.channels) inputs and (None, 1) output - one input and one sequential object\n",
        "        self.discriminator_model.compile(loss=HPARAM.loss, optimizer=HPARAM.optimizer('Adam'), metrics=HPARAM.metrics) # binary crossentropy between an output (predicted y) tensor and a target (real y) tensor since the output of the discriminator is sigmoid \n",
        "        self.discriminator_model.summary()\n",
        "        print(f\"\\n\\n\\t\\t [======Discriminator Tensor======] \\n\\n\\t\\t {discriminator_tensor}\\n\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2wGYDhAQIXl",
        "colab_type": "text"
      },
      "source": [
        "# **Creating & Compiling Adversarial Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypMSG4yCQMan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def __create_networks(self):\n",
        "        '''\n",
        "        We now create the GAN where we combine the Generator and discriminator. \n",
        "        When we train the generator we will freeze the discriminator model.\n",
        "        \n",
        "        We will input the noised image of shape for example 100 units to the generator. \n",
        "        The output generated from the generator will be fed to the discriminator.\n",
        "        '''\n",
        "        self.__MakeGeneratorModel()\n",
        "        self.__MakeDiscriminatorModel()\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ GAN SUMMARY +)+)+)+)+)+) \\n\\n\")\n",
        "        self.discriminator_model.trainable = False # freeze the model because at first, we will train only generator model.\n",
        "        real_input = Input(shape=(self.generator_input_features,)) # the real input features of our gan model \n",
        "        generator_output_tensor = self.generator_model(real_input) # pass input of shape for example 100 neurons to generator model input - output size : (None, self.h, self.w, self.channels)\n",
        "        discriminator_output_tensor = self.discriminator_model(generator_output_tensor) # this is the output tensor of our discriminator model which is the result of passing the output of generator model to it for discriminating - output size : (None , 1)\n",
        "        self.gan = Model(inputs=real_input, outputs=discriminator_output_tensor) # input size : (None, 100) - output size : (None, self.h, self.w, self.channels) and (None, 1) for two model objects\n",
        "        self.gan.compile(loss=HPARAM.loss, optimizer=HPARAM.optimizer('Adam')) # use Adam optimizer to prevent nan loss from happening!\n",
        "        self.gan.summary() # the structure is : one input layer and 2 model objects | data -> generator -> discriminator .... gan(x) = discriminator(generator(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-p-r4epQZiL",
        "colab_type": "text"
      },
      "source": [
        "# **Tooling Methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv8CH1MkQg4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def __squash(self, vectors, axis=-1):\n",
        "        '''\n",
        "        It drives the length of a large vector to near 1 and small vector to 0.\n",
        "        is used to normalize the magnitude of vectors, rather than the scalar elements themselves.\n",
        "        the epsilon is a small floating point number used to generally avoid mistakes like divide by zero. \n",
        "        \n",
        "        vj=∥sj∥2/1+∥sj∥2*sj/∥sj∥\n",
        "        '''\n",
        "        s_squared_sum = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "        scale = s_squared_sum / (1 + s_squared_sum) / K.sqrt(s_squared_sum + K.epsilon())\n",
        "        return scale * vectors # a tensor with same shape as input vectors because of keepdims flag\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def predictNoise(self, b_size):\n",
        "    # noise = tf.random.normal([b_size, self.generator_input_features])\n",
        "    # return self.generator_model.predict(noise, steps=b_size)\n",
        "    noise = np.random.normal(0, 1, (b_size, self.generator_input_features)) # output shape : (25,100) - to match the first layer matrix we suppose a (25, 100) matrix ; cause our first layer has 100 features or neurons | random vector from the latent space\n",
        "    generated_noise = self.generator_model.predict(noise) # input shape : (b_size, 100) to the generator model with 12544 neurons for first hidden layer\n",
        "    # print(f\"\\n\\n[======NONE SCALED GENERATED NOISE======]\\n\\n{generated_noise}\") # the are between [-1, 1]\n",
        "    generated_noise = 0.5 * generated_noise + 0.5 # scale the image which is between -1 and 1 to 0 and 1 - because the output of discriminator is [0, 1] and we have to scale our input data for the network\n",
        "    # print(f\"\\n\\n[======SCALED GENERATED NOISE======]\\n\\n{generated_noise}\")\n",
        "    return generated_noise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def __plotLoss(self, analaysis):\n",
        "    anal = pd.DataFrame(analaysis)\n",
        "    # print(f\"\\n\\n[=========ANALAYSIS DATAFRAME=========]\\n\\n\\t{analaysis}\\n\\n\")\n",
        "    plt.figure(figsize=(20,5))\n",
        "    for col in anal.columns:\n",
        "        plt.plot(anal[col], label=col)\n",
        "    plt.legend()\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def saveModels(self):\n",
        "    self.discriminator_model.save('/gdrive/My Drive/GAN-models/disc.h5')\n",
        "    self.generator_model.save('/gdrive/My Drive/GAN-models/gen.h5')\n",
        "    self.gan.save('/gdrive/My Drive/GAN-models/gan.h5')\n",
        "            \n",
        "\n",
        "\n",
        "def __saveImages(self, epoch):\n",
        "    generated_noise = self.predictNoise(b_size=25) # predict for 25 noisy images or 25 batch size - output size : (25, self.h, self.w, self.channels)\n",
        "    self.ipkit.saveGANIMG(generated_noise, epoch, self.dataset_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kod6QPRaRJBI",
        "colab_type": "text"
      },
      "source": [
        "# **Training Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbEHiDRJKdq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(self):\n",
        "    '''\n",
        "    G(Z) : generated_noise & D(G(Z)) : discriminating generated_noise.\n",
        "\n",
        "    since we are only training generators here, we do not want to adjust the weights of discriminator.\n",
        "    this is what really an “Adversarial” in Adversarial Network means, if we do not set this,\n",
        "    the generator will get its weight adjusted so it gets better at fooling discriminator \n",
        "    and it also adjusts the weights of the discriminator to make it better at being fooled.\n",
        "    we don’t want this. So, we have to train them separately and fight against each other.\n",
        "\n",
        "    NOTE : for weights matrix of for example discriminator model you might want look at the self.discriminator_model.trainable_weights\n",
        "    '''\n",
        "    real, fake, analaysis = np.ones((HPARAM.batch_size, 1)), np.zeros((HPARAM.batch_size, 1)), []\n",
        "    for epoch in range(HPARAM.epochs):\n",
        "        # train the discriminator\n",
        "        if self.dataset_name == 'paint_art':\n",
        "            batch_indices = np.random.randint(0, len(self.x_train))\n",
        "        else:\n",
        "            batch_indices = np.random.randint(0, self.x_train.shape[0], HPARAM.batch_size) # select a random batch index in every epoch - from 0 to 60000 select HPARAM.batch_size numbers (all in a vector) randomly\n",
        "        batch = self.x_train[batch_indices] # get a random set of real images - shape for all dataset except paint_art : (256, self.h, self.w, self.channels)\n",
        "        batch = 0.5 * batch + 0.5 # rescale to [0, 1] - because all training images have range [-1, 1] and to feed the batch into the discriminator network we have to scale our data to [0, 1]\n",
        "        generated_noise = self.predictNoise(HPARAM.batch_size) # for 256 data we produce noisy images using our generator model with shape (256, self.h, self.w, self.channels)\n",
        "        self.discriminator_model.trainable = True # pre train discriminator on fake and real data before starting the gan to let the discriminator model weights update\n",
        "        real_metric_loss = self.discriminator_model.train_on_batch(batch, real) # runs a single gradient update on a single batch of data and returns scalar training loss for real images - how much they are real! train to get the 1s.\n",
        "        fake_metric_loss = self.discriminator_model.train_on_batch(generated_noise, fake)  # runs a single gradient update on a single batch of data and returns scalar training loss for fake images - how much they are fake! train to get the 0s.\n",
        "        discriminator_loss = 0.5 * np.add(real_metric_loss, fake_metric_loss) # in practice, we divide the objective by 2 while optimizing discriminator, which slows down the rate at which discriminator learns relatively to generator.\n",
        "        # train the generator\n",
        "        self.discriminator_model.trainable = False # during the training of gan, the weights of discriminator should be fixed and we can enforce that by setting the trainable flag\n",
        "        noise = np.random.normal(0, 1, (HPARAM.batch_size, self.generator_input_features)) # we'll feed this generated noise into our gan model to produce real images from noisy by training our gen model\n",
        "        generator_metric_loss = self.gan.train_on_batch(noise, real) # training the gan by alternating the training of the discriminator and training the chained gan model with discriminator’s weights freezed ; closing predicted noise from generator to real labels-  runs a single gradient update on a single batch of data and returns scalar training loss - take a (batch_size, 100) matrix as input and (batch_size, 1) filled with 1 matrix as real output value ; our gan model has (batch_size, 100) -> (batch_size, 28, 82, 1) -> (batch_size, 1) architecture\n",
        "        \n",
        "        print(f\"[*************EPOCH - {epoch + 1}*************]\")\n",
        "        print(f\"DISCRIMINATOR LOSS ⏎\\n\\t{discriminator_loss[0]}\\n\")\n",
        "        print(f\"DISCRIMINATOR ACC ⏎\\n\\t{discriminator_loss[1]*100}\\n\")\n",
        "        print(f\"GENERATOR LOSS ⏎\\n\\t{generator_metric_loss}\\n\")\n",
        "        print(\"_________________________________________________________________________________________________________\\n\")\n",
        "        \n",
        "        analaysis.append({\"D\": discriminator_loss[0], \"G\": generator_metric_loss})\n",
        "        if epoch % 10 == 0:\n",
        "            self.__saveImages(epoch)\n",
        "    self.__plotLoss(analaysis)\n",
        "    self.ipkit.MakeGif()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhiJ4wpWSX5W",
        "colab_type": "text"
      },
      "source": [
        "# **Testing our GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHwb9ruOSnuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# discriminator network : dcgan or cgan | dataset_name : paint_art, fashion_mnist, mnist, cifar20 | channels : 1 for mnist and fashion_mnist, 3 for paint_art and cifar10\n",
        "gan = GAN(dataset_name='paint_art', channels=3, generator_input_features=100, discNetwork='dcgan')\n",
        "gan.fit() # start training\n",
        "gan.saveModels() # save trained models\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QJvyyAaotS1",
        "colab_type": "text"
      },
      "source": [
        "**Deep Convolutional GAN Generated Noise After 30K Epochs On CIFAR-10 Dataset**\n",
        "\n",
        "![dcgan gif cifar10]()\n",
        "\n",
        "**Capsule GAN Generated Noise After 30K Epochs On CIFAR-10 Dataset**\n",
        "\n",
        "![cgan gif cifar10]()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoD-G99BYZ4",
        "colab_type": "text"
      },
      "source": [
        "**GAN Prediction API - Tensorflow Serving**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uRwyvHFBXJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# TODO : use tensorboard to evaluate models\n",
        "\n",
        "gen = load_model('/gdrive/My Drive/GAN-models/gen.h5')\n",
        "noise = np.random.normal(0, 1, (25, gan.generator_input_features))\n",
        "predicted_noise = gen.predict(noise) # it should give us a real image!\n",
        "if gan.dataset_name == 'cifar10' or gan.dataset_name == 'paint_art':\n",
        "    plt.imshow((predicted_noise[0, :, :, :] * 127.5 + 127.5).astype(np.uint8)) # plot the 0th predicted noise - because of generator output we have to scale the prediction to [0, 255], so we multiply by 127.5 and add 127.5\n",
        "else:\n",
        "    plt.imshow(predicted_noise[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "predicted_noise = 0.5 * predicted_noise + 0.5 # because the discriminator output is in range [0, 1] we have to scale the generated noise\n",
        "print(\"\\n\\n |=> 0th GENERATED NOISE FROM GENERATOR <=|\")\n",
        "disc = load_model('/gdrive/My Drive/GAN-models/disc.h5')\n",
        "print(\"\\n\\n |=> DISCRIMINATING 25 GENRATED NOISE BATCHES <=|\\n\\n{}\".format(disc.predict(predicted_noise)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}